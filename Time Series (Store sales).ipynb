{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-09T18:31:55.551241Z","iopub.execute_input":"2024-03-09T18:31:55.551752Z","iopub.status.idle":"2024-03-09T18:31:56.858101Z","shell.execute_reply.started":"2024-03-09T18:31:55.551721Z","shell.execute_reply":"2024-03-09T18:31:56.857379Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/store-sales-time-series-forecasting/oil.csv\n/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv\n/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\n/kaggle/input/store-sales-time-series-forecasting/stores.csv\n/kaggle/input/store-sales-time-series-forecasting/train.csv\n/kaggle/input/store-sales-time-series-forecasting/test.csv\n/kaggle/input/store-sales-time-series-forecasting/transactions.csv\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_13/1229163428.py:6: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Introduction\n\nAssalamu alaikum everyone, this time our task is to create a model that does forecasting on sales in store in Ecuadorian.\n\nLet's start with importing all libraries those we need.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:33:51.739713Z","iopub.execute_input":"2024-03-09T18:33:51.740071Z","iopub.status.idle":"2024-03-09T18:33:53.804256Z","shell.execute_reply.started":"2024-03-09T18:33:51.740042Z","shell.execute_reply":"2024-03-09T18:33:53.803562Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Dowloading data and creating data frames.\n\nAs you can see whole bunch of data. We should creat dataframe from each csv file and then convert date column to datatime from object.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\ntrain['date'] = pd.to_datetime(train['date'])\n\ntest  = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\ntest['date'] = pd.to_datetime(test['date'])\n\noil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')\noil['date'] = pd.to_datetime(oil['date'])\n\nholidays_events = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')\nholidays_events['date'] = pd.to_datetime(holidays_events['date'])\nholidays_events.rename(columns={'type': 'holiday_type'}, inplace=True)\n\nstores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\nstores.rename(columns={'type': 'store_type'}, inplace=True)\n\ntransactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\ntransactions['date'] = pd.to_datetime(transactions['date'])","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:33:57.783032Z","iopub.execute_input":"2024-03-09T18:33:57.783706Z","iopub.status.idle":"2024-03-09T18:34:00.830907Z","shell.execute_reply.started":"2024-03-09T18:33:57.783672Z","shell.execute_reply":"2024-03-09T18:34:00.830205Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Creating new features.\n\nIn this cell here we are creating new featuers from date column, which are year, month, day, day of week, weekend. These features will be very usefull for our model.","metadata":{}},{"cell_type":"code","source":"train['year'] = train['date'].dt.year\ntrain['month'] = train['date'].dt.month\ntrain['day'] = train['date'].dt.day\ntrain['day_of_week'] = train['date'].dt.dayofweek\ntrain['weekend'] = train['day_of_week'].isin([5, 6]).astype(int)\n\ntest['year'] = test['date'].dt.year\ntest['month'] = test['date'].dt.month\ntest['day'] = test['date'].dt.day\ntest['day_of_week'] = test['date'].dt.dayofweek\ntest['weekend'] = test['day_of_week'].isin([5, 6]).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:04.452931Z","iopub.execute_input":"2024-03-09T18:34:04.453236Z","iopub.status.idle":"2024-03-09T18:34:04.781798Z","shell.execute_reply.started":"2024-03-09T18:34:04.453210Z","shell.execute_reply":"2024-03-09T18:34:04.781042Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"In Ecuador public sector pays the sales 2 times in a month, 15th day and the last day. So in following cell we will create column called 'payment_day', which describes wheater it is payment day.","metadata":{}},{"cell_type":"code","source":"train['payment_day'] = train['date'].apply(lambda x: 1 if x.day == 15 or x.is_month_end else 0)\n\ntest['payment_day'] = test['date'].apply(lambda x: 1 if x.day == 15 or x.is_month_end else 0)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:07.076743Z","iopub.execute_input":"2024-03-09T18:34:07.077148Z","iopub.status.idle":"2024-03-09T18:34:13.279483Z","shell.execute_reply.started":"2024-03-09T18:34:07.077096Z","shell.execute_reply":"2024-03-09T18:34:13.278420Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"A magnitude 7.8 earthquake struck Ecuador on April 16, 2016, so we create a feature informs about earthquake.","metadata":{}},{"cell_type":"code","source":"earth_quake = pd.to_datetime('2016-04-16 00:00:00')\n\ntrain['after_earthquake'] = train['date'].apply(lambda x: 1 if x >= earth_quake else 1)\n\ntest['after_earthquake'] = test['date'].apply(lambda x: 1 if x >= earth_quake else 0)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:15.578263Z","iopub.execute_input":"2024-03-09T18:34:15.578738Z","iopub.status.idle":"2024-03-09T18:34:21.227011Z","shell.execute_reply.started":"2024-03-09T18:34:15.578702Z","shell.execute_reply":"2024-03-09T18:34:21.225985Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Merging main data to other datas.\n\nIn the following cells we will merge our main data with other data frames.","metadata":{}},{"cell_type":"code","source":"train = pd.merge(train, oil, on='date', how='left')\ntrain['dcoilwtico'] = train['dcoilwtico'].fillna(method='backfill')\n\ntest = pd.merge(test, oil, on='date', how='left')\ntest['dcoilwtico'] = test['dcoilwtico'].fillna(method='backfill')","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:23.402756Z","iopub.execute_input":"2024-03-09T18:34:23.403160Z","iopub.status.idle":"2024-03-09T18:34:23.798775Z","shell.execute_reply.started":"2024-03-09T18:34:23.403126Z","shell.execute_reply":"2024-03-09T18:34:23.797889Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_13/3920736263.py:2: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  train['dcoilwtico'] = train['dcoilwtico'].fillna(method='backfill')\n/tmp/ipykernel_13/3920736263.py:5: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  test['dcoilwtico'] = test['dcoilwtico'].fillna(method='backfill')\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.merge(train, holidays_events, how='left', on='date')\n\ntest = pd.merge(test, holidays_events, how='left', on='date')","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:26.643292Z","iopub.execute_input":"2024-03-09T18:34:26.643692Z","iopub.status.idle":"2024-03-09T18:34:27.902512Z","shell.execute_reply.started":"2024-03-09T18:34:26.643660Z","shell.execute_reply":"2024-03-09T18:34:27.901602Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train, stores, how='left', on='store_nbr')\n\ntest = pd.merge(test, stores, how='left', on='store_nbr')","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:29.163354Z","iopub.execute_input":"2024-03-09T18:34:29.163775Z","iopub.status.idle":"2024-03-09T18:34:30.370630Z","shell.execute_reply.started":"2024-03-09T18:34:29.163741Z","shell.execute_reply":"2024-03-09T18:34:30.369475Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train, transactions, how='left', on=['date', 'store_nbr'])\n\ntest = pd.merge(test, transactions, how='left', on=['date', 'store_nbr'])","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:32.311144Z","iopub.execute_input":"2024-03-09T18:34:32.311583Z","iopub.status.idle":"2024-03-09T18:34:33.915753Z","shell.execute_reply.started":"2024-03-09T18:34:32.311548Z","shell.execute_reply":"2024-03-09T18:34:33.914573Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train.drop(['date'], axis=1, inplace=True)\n\ntest.drop(['date'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:35.354740Z","iopub.execute_input":"2024-03-09T18:34:35.355131Z","iopub.status.idle":"2024-03-09T18:34:35.783470Z","shell.execute_reply.started":"2024-03-09T18:34:35.355100Z","shell.execute_reply":"2024-03-09T18:34:35.782567Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Label Encoding and Standart Scaling.","metadata":{}},{"cell_type":"code","source":"la = LabelEncoder()\ncat_cols = ['family', 'holiday_type', 'locale', 'locale_name', 'transferred', 'city', 'state', 'store_type', 'description']\n\n# Apply LabelEncoder to each categorical column separately\nfor col in cat_cols:\n    train[col] = la.fit_transform(train[col])\n    test[col] = la.transform(test[col])\n\n# List of numerical columns\nnumerical_cols = ['year', 'cluster', 'day_of_week', 'month', 'weekend', 'payment_day', 'onpromotion', 'store_nbr', 'after_earthquake', 'transactions', 'dcoilwtico', 'day']\n\n# Initialize StandardScaler\nscaler = StandardScaler()\n\n# Apply StandardScaler to the specified columns in train DataFrame\ntrain[numerical_cols] = scaler.fit_transform(train[numerical_cols])\n\n# Apply the same transformation to the specified columns in the test DataFrame\ntest[numerical_cols] = scaler.transform(test[numerical_cols])","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:37.933855Z","iopub.execute_input":"2024-03-09T18:34:37.934250Z","iopub.status.idle":"2024-03-09T18:34:43.597870Z","shell.execute_reply.started":"2024-03-09T18:34:37.934219Z","shell.execute_reply":"2024-03-09T18:34:43.596989Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Spitting data.","metadata":{}},{"cell_type":"code","source":"X = train.drop(['sales'], axis=1)\ny = train['sales']","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:46.594144Z","iopub.execute_input":"2024-03-09T18:34:46.594980Z","iopub.status.idle":"2024-03-09T18:34:46.908349Z","shell.execute_reply.started":"2024-03-09T18:34:46.594942Z","shell.execute_reply":"2024-03-09T18:34:46.907362Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:34:49.483205Z","iopub.execute_input":"2024-03-09T18:34:49.484089Z","iopub.status.idle":"2024-03-09T18:34:50.661565Z","shell.execute_reply.started":"2024-03-09T18:34:49.484050Z","shell.execute_reply":"2024-03-09T18:34:50.660654Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Training a model.\n\nI chose XGB Regressor to our task.","metadata":{}},{"cell_type":"code","source":"# Specify additional parameters\nparams = {\n    'n_estimators': 1000,  # 1000 \n    'max_depth': 20,       # 20\n    'learning_rate': 0.1,  # You can adjust the learning rate\n    'subsample': 0.8,      # Fraction of samples used for fitting the individual base learners\n    'colsample_bytree': 0.8,  # Fraction of features used for fitting the individual base learners\n    'gamma': 0,            # Minimum loss reduction required to make a further partition on a leaf node of the tree\n    'objective': 'reg:squarederror',  # Specify the learning task and the corresponding objective function\n    'reg_alpha': 0,        # L1 regularization term on weights\n    'reg_lambda': 1,       # L2 regularization term on weights\n    'random_state': 42,\n    'n_jobs': -1\n}\n\n# Create XGBRegressor with specified parameters\nxg_reg = xgb.XGBRegressor(**params)\nxg_reg.fit(X_train, y_train)\nY_pred = xg_reg.predict(X_test)\n\nsquared_log_errors = (np.log1p(Y_pred) - np.log1p(y_test)) ** 2\nmean_squared_log_error = np.mean(squared_log_errors)\nrmsle = np.sqrt(mean_squared_log_error)\nprint(\"RMSLE:\", rmsle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing csv file to submission.","metadata":{}},{"cell_type":"code","source":"test_pred = xg_reg.predict(test.drop('id', axis=1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id': test['id'], 'sales': test_pred})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}
